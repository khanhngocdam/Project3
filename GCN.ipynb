{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 591574 entries, 0 to 591573\n",
      "Data columns (total 85 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   FlowID                   591570 non-null  object \n",
      " 1   SourceIP                 591574 non-null  object \n",
      " 2   SourcePort               591574 non-null  float64\n",
      " 3   DestinationIP            591574 non-null  object \n",
      " 4   DestinationPort          591574 non-null  float64\n",
      " 5   Protocol                 591574 non-null  float64\n",
      " 6   Timestamp                591574 non-null  object \n",
      " 7   FlowDuration             591574 non-null  float64\n",
      " 8   TotalFwdPackets          591574 non-null  float64\n",
      " 9   TotalBackwardPackets     591574 non-null  float64\n",
      " 10  TotalLengthofFwdPackets  591574 non-null  float64\n",
      " 11  TotalLengthofBwdPackets  591574 non-null  float64\n",
      " 12  FwdPacketLengthMax       591574 non-null  float64\n",
      " 13  FwdPacketLengthMin       591574 non-null  float64\n",
      " 14  FwdPacketLengthMean      591574 non-null  float64\n",
      " 15  FwdPacketLengthStd       591574 non-null  float64\n",
      " 16  BwdPacketLengthMax       591574 non-null  float64\n",
      " 17  BwdPacketLengthMin       591574 non-null  float64\n",
      " 18  BwdPacketLengthMean      591574 non-null  float64\n",
      " 19  BwdPacketLengthStd       591574 non-null  float64\n",
      " 20  FlowBytes/s              591574 non-null  float64\n",
      " 21  FlowPackets/s            591574 non-null  float64\n",
      " 22  FlowIATMean              591573 non-null  float64\n",
      " 23  FlowIATStd               591573 non-null  float64\n",
      " 24  FlowIATMax               591573 non-null  float64\n",
      " 25  FlowIATMin               591573 non-null  float64\n",
      " 26  FwdIATTotal              591573 non-null  float64\n",
      " 27  FwdIATMean               591573 non-null  float64\n",
      " 28  FwdIATStd                591573 non-null  float64\n",
      " 29  FwdIATMax                591573 non-null  float64\n",
      " 30  FwdIATMin                591573 non-null  float64\n",
      " 31  BwdIATTotal              591573 non-null  float64\n",
      " 32  BwdIATMean               591573 non-null  float64\n",
      " 33  BwdIATStd                591573 non-null  float64\n",
      " 34  BwdIATMax                591572 non-null  float64\n",
      " 35  BwdIATMin                591572 non-null  float64\n",
      " 36  FwdPSHFlags              591572 non-null  float64\n",
      " 37  BwdPSHFlags              591572 non-null  float64\n",
      " 38  FwdURGFlags              591572 non-null  float64\n",
      " 39  BwdURGFlags              591572 non-null  float64\n",
      " 40  FwdHeaderLength          591572 non-null  float64\n",
      " 41  BwdHeaderLength          591572 non-null  float64\n",
      " 42  FwdPackets/s             591572 non-null  float64\n",
      " 43  BwdPackets/s             591571 non-null  float64\n",
      " 44  MinPacketLength          591571 non-null  float64\n",
      " 45  MaxPacketLength          591571 non-null  float64\n",
      " 46  PacketLengthMean         591571 non-null  float64\n",
      " 47  PacketLengthStd          591571 non-null  object \n",
      " 48  PacketLengthVariance     591570 non-null  float64\n",
      " 49  FINFlagCount             591570 non-null  float64\n",
      " 50  SYNFlagCount             591570 non-null  float64\n",
      " 51  RSTFlagCount             591570 non-null  float64\n",
      " 52  PSHFlagCount             591570 non-null  float64\n",
      " 53  ACKFlagCount             591570 non-null  float64\n",
      " 54  URGFlagCount             591570 non-null  float64\n",
      " 55  CWEFlagCount             591570 non-null  object \n",
      " 56  ECEFlagCount             591569 non-null  float64\n",
      " 57  Down/UpRatio             591569 non-null  object \n",
      " 58  AveragePacketSize        591568 non-null  float64\n",
      " 59  AvgFwdSegmentSize        591568 non-null  float64\n",
      " 60  AvgBwdSegmentSize        591568 non-null  float64\n",
      " 61  FwdHeaderLength.1        591568 non-null  float64\n",
      " 62  FwdAvgBytes/Bulk         591568 non-null  float64\n",
      " 63  FwdAvgPackets/Bulk       591568 non-null  float64\n",
      " 64  FwdAvgBulkRate           591568 non-null  float64\n",
      " 65  BwdAvgBytes/Bulk         591568 non-null  float64\n",
      " 66  BwdAvgPackets/Bulk       591568 non-null  float64\n",
      " 67  BwdAvgBulkRate           591568 non-null  float64\n",
      " 68  SubflowFwdPackets        591568 non-null  float64\n",
      " 69  SubflowFwdBytes          591568 non-null  float64\n",
      " 70  SubflowBwdPackets        591568 non-null  float64\n",
      " 71  SubflowBwdBytes          591568 non-null  float64\n",
      " 72  Init_Win_bytes_forward   591568 non-null  float64\n",
      " 73  Init_Win_bytes_backward  591568 non-null  float64\n",
      " 74  act_data_pkt_fwd         591568 non-null  float64\n",
      " 75  min_seg_size_forward     591568 non-null  float64\n",
      " 76  ActiveMean               591568 non-null  float64\n",
      " 77  ActiveStd                591567 non-null  float64\n",
      " 78  ActiveMax                591567 non-null  float64\n",
      " 79  ActiveMin                591567 non-null  float64\n",
      " 80  IdleMean                 591567 non-null  float64\n",
      " 81  IdleStd                  591567 non-null  float64\n",
      " 82  IdleMax                  591567 non-null  float64\n",
      " 83  IdleMin                  591567 non-null  float64\n",
      " 84  Label                    591574 non-null  int64  \n",
      "dtypes: float64(77), int64(1), object(7)\n",
      "memory usage: 383.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Đường dẫn đến folder chứa các file CSV\n",
    "list_folder_malware = ['./CSVS/SMSmalware-CSVs/SMSmalware/Biige', './CSVS/SMSmalware-CSVs/SMSmalware/Fakeinst', './CSVS/SMSmalware-CSVs/SMSmalware/FakeNotify',\n",
    "                       './CSVS/SMSmalware-CSVs/SMSmalware/Mazarbot', './CSVS/SMSmalware-CSVs/SMSmalware/Jifake', './CSVS/SMSmalware-CSVs/SMSmalware/Nandrobox',\n",
    "                       './CSVS/SMSmalware-CSVs/SMSmalware/Plankton', './CSVS/SMSmalware-CSVs/SMSmalware/Zsone']\n",
    "\n",
    "list_folder_benign = ['./CSVS/Benign-CSVs/Benign/Benign2017']\n",
    "\n",
    "\n",
    "# Tạo một DataFrame để chứa dữ liệu từ tất cả các file CSV\n",
    "list_df = []\n",
    "all_data = pd.DataFrame()\n",
    "#Đọc từng file CSV và nối dữ liệu vào DataFrame chung\n",
    "i = 0\n",
    "# Đọc benign\n",
    "for folder in list_folder_benign: \n",
    "     file_list =  os.listdir(folder)\n",
    "     csv_files = [file for file in file_list if file.endswith('.csv')]\n",
    "     for csv in csv_files: \n",
    "            file_path = os.path.join(folder, csv)\n",
    "            data = pd.read_csv(file_path, header=0)\n",
    "            data.columns = data.columns.str.replace(' ', '')\n",
    "            data = data.sort_values(by='Timestamp')\n",
    "            data['Label'] = 1\n",
    "            list_df.append(data)\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "            i = i + 1\n",
    "# Đọc malware\n",
    "for folder in list_folder_malware: \n",
    "     file_list =  os.listdir(folder)\n",
    "     csv_files = [file for file in file_list if file.endswith('.csv')]\n",
    "     for csv in csv_files: \n",
    "        file_path = os.path.join(folder, csv)\n",
    "        data = pd.read_csv(file_path, header=0)\n",
    "        data.columns = data.columns.str.replace(' ', '')\n",
    "        data = data.sort_values(by='Timestamp')\n",
    "        data['Label'] = 0\n",
    "        list_df.append(data)\n",
    "        all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hiển thị DataFrame chứa dữ liệu từ tất cả các file CSV\n",
    "print(all_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "1    410548\n",
       "0    181026\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label                      1.000000\n",
       "ACKFlagCount               0.026583\n",
       "URGFlagCount               0.022652\n",
       "Init_Win_bytes_backward    0.012253\n",
       "SubflowFwdPackets          0.004621\n",
       "                             ...   \n",
       "FlowIATMax                -0.041337\n",
       "FlowDuration              -0.044689\n",
       "FwdIATStd                 -0.047722\n",
       "FwdIATTotal               -0.054961\n",
       "FwdIATMax                 -0.055962\n",
       "Name: Label, Length: 65, dtype: float64"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "columns_drop=['FlowID', 'SourceIP', 'DestinationIP', 'SourcePort', 'DestinationPort', 'Timestamp',\n",
    "                               'PacketLengthStd', 'CWEFlagCount', 'Down/UpRatio','FwdAvgPackets/Bulk', 'FwdAvgBulkRate', \n",
    "                               'BwdAvgBytes/Bulk', 'BwdAvgPackets/Bulk', 'BwdAvgBulkRate', 'FwdURGFlags', 'BwdURGFlags', \n",
    "                               'RSTFlagCount', 'ECEFlagCount', 'BwdPSHFlags', 'FwdAvgBytes/Bulk']\n",
    "# Tính ma trận tương quan\n",
    "data = all_data.drop(columns=columns_drop)\n",
    "corr_matrix = data.corr()\n",
    "corr_matrix['Label'].sort_values(ascending=False)\n",
    "\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_valid_format(timestamp, format='%d/%m/%Y %H:%M:%S'):\n",
    "#     try:\n",
    "#         pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M:%S')\n",
    "#         return True\n",
    "#     except ValueError:\n",
    "#         return False\n",
    "    \n",
    "# list_df_5_minutes = []\n",
    "# #Tạo list graph\n",
    "# for df in list_df: \n",
    "#     i = 0\n",
    "#     timeStamp = df['Timestamp'].iloc[i] \n",
    "#     # Chuyển đổi timestamp thành đối tượng datetime và cộng 5 phút\n",
    "    \n",
    "#     while not is_valid_format(timeStamp, format='%d/%m/%Y %H:%M:%S') :\n",
    "#         i = i + 1\n",
    "#         timeStamp = df['Timestamp'].iloc[i]\n",
    "\n",
    "#     datetime = pd.to_datetime(timeStamp, format='%d/%m/%Y %H:%M:%S') + pd.Timedelta(minutes=5)\n",
    "#     new_df = pd.DataFrame(columns=all_data.columns)\n",
    "#     for row in df.itertuples(index=False):\n",
    "#         try:\n",
    "#             date_time_of_row = pd.to_datetime(row.Timestamp, format='%d/%m/%Y %H:%M:%S')\n",
    "#         except ValueError:\n",
    "#             # Bỏ qua row nếu không thể chuyển đổi thành đối tượng datetime\n",
    "#             continue\n",
    "#         if(date_time_of_row <= datetime) : \n",
    "#             new_df.loc[len(new_df)] = list(row)\n",
    "#         elif not new_df.empty: \n",
    "#             list_df_5_minutes.append(new_df)\n",
    "#             new_df = pd.DataFrame(columns=all_data.columns)\n",
    "#             datetime = datetime + pd.Timedelta(minutes=5)\n",
    "\n",
    "# len(list_df_5_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ip_label(df):\n",
    "    le_columns = ['SourceIP', 'DestinationIP']\n",
    "    ip_list = list(df['SourceIP'].unique()) + list(df['DestinationIP'].unique())\n",
    "    ip_set = list(set(ip_list))\n",
    "\n",
    "    for column in le_columns:\n",
    "        list_unique = list(df[column].unique())\n",
    "        for val in list_unique:\n",
    "            df.loc[df[column] == val, column] = ip_set.index(val)\n",
    "\n",
    "    df['DestinationIP'] = df['DestinationIP'].astype(int)\n",
    "    df['SourceIP'] = df['SourceIP'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "list_graph = []\n",
    "list_label = []\n",
    "for df in list_df:\n",
    "    df = convert_ip_label(df)\n",
    "    graph = nx.from_pandas_edgelist(df, 'SourceIP', 'DestinationIP',\n",
    "                                     create_using=nx.MultiDiGraph(), edge_attr=df.drop(columns=columns_drop).columns.values.tolist())\n",
    "    list_graph.append(graph)\n",
    "    count_value = len(df['Label'].unique())\n",
    "    if count_value == 1: \n",
    "        label = df['Label'].unique()[0]\n",
    "    else: \n",
    "        label = 0\n",
    "    list_label.append(label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.loader import  DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "def graph_to_pyg_data(graph, label):\n",
    "    # Khởi tạo ma trận B_in với tất cả các phần tử bằng 0\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    B_in = torch.zeros((num_nodes, num_edges), dtype=torch.float32)\n",
    "    B_out = torch.zeros((num_nodes, num_edges), dtype=torch.float32)\n",
    "    for i, node in enumerate(graph.nodes):\n",
    "        for j, edge in enumerate(graph.edges):\n",
    "            if node == edge[1]:\n",
    "                B_in[i, j] = 1\n",
    "            if node == edge[0]:\n",
    "                B_out[i, j] = 1\n",
    "    Y = torch.tensor(label, dtype=torch.long)\n",
    "    X = torch.tensor([list(graph.edges[edge].values()) for edge in graph.edges], dtype=torch.float)\n",
    "    return Data(B_in = B_in, B_out=B_out, X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_graph_labeled = list(zip(list_graph, list_label))\n",
    "list_data = []\n",
    "for graph, label in list_graph_labeled:\n",
    "    list_data.append(graph_to_pyg_data(graph, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "        self.fc0 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(3*hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, B_in, B_out, X):\n",
    "        # Tính E_0\n",
    "        E_0 = F.relu(self.fc0(X))\n",
    "        # Tính H0\n",
    "        matrix1 = torch.matmul(B_in, E_0)\n",
    "        matrix2 = torch.matmul(B_out, E_0)\n",
    "        result_matrix_0 = torch.cat((matrix1, matrix2), dim=1)\n",
    "        H_0 = F.relu(self.fc1(result_matrix_0))\n",
    "        # Tính E1\n",
    "        matrix3 = torch.matmul(B_in.t(), H_0)\n",
    "        matrix4 = torch.matmul(B_out.t(), H_0)\n",
    "        result_matrix_1 = torch.cat((matrix3, matrix4, E_0), dim=1)\n",
    "        E_1 = F.relu(self.fc2(result_matrix_1))\n",
    "        # Tính H1\n",
    "        matrix5 = torch.matmul(B_in, E_1)\n",
    "        matrix6 = torch.matmul(B_out, E_1)\n",
    "        result_matrix_2 = torch.cat((matrix5, matrix6, H_0), dim=1)\n",
    "        H_1 = F.relu(self.fc2(result_matrix_2))\n",
    "        # Pooling mean cho H_1\n",
    "        H_1_mean = torch.mean(H_1, dim=0, keepdim=True)\n",
    "        # Softmax cho đầu ra của H_1_mean\n",
    "        output = F.softmax(self.fc3(H_1_mean), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "train_data, test_data = train_test_split(list_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, list_data):\n",
    "#         self.list_data = list_data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.list_data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if isinstance(idx, torch.Tensor):\n",
    "#             idx = idx.tolist()\n",
    "#         B_in = self.list_data[idx].B_in\n",
    "#         B_out = self.list_data[idx].B_out\n",
    "#         X = self.list_data[idx].X\n",
    "#         Y = self.list_data[idx].Y\n",
    "\n",
    "#         return [B_in, B_out, X, Y]\n",
    "    \n",
    "# def my_collate(batch):\n",
    "#     inputs, labels = zip(*batch)\n",
    "#     return inputs, labels\n",
    "\n",
    "# trainset = MyDataset(train_data)\n",
    "# testset = MyDataset(test_data)\n",
    "# train_loader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=my_collate)\n",
    "# test_loader = DataLoader(testset, batch_size=64, shuffle=False, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "input_dim = 65 # Thay thế bằng kích thước của vectơ đặc trưng đỉnh\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "lr = 0.001\n",
    "# Use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GraphNeuralNetwork(input_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Huấn luyện mô hình\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.B_in, data.B_out, data.X)\n",
    "        if torch.isnan(output).any():\n",
    "            continue\n",
    "        loss = criterion(output.squeeze(), data.Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Đánh giá mô hình\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            output = model(data.B_in, data.B_out, data.X)\n",
    "            if torch.isnan(output).any():\n",
    "                continue\n",
    "            loss = criterion(output, data.Y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == data.Y).sum().item()\n",
    "            total_samples += 1\n",
    "\n",
    "    accuracy = correct / total_samples\n",
    "    average_loss = total_loss / len(loader)\n",
    "\n",
    "    return accuracy, average_loss\n",
    "\n",
    "# Thiết lập các tham số\n",
    "input_dim = 65 # Thay thế bằng kích thước của vectơ đặc trưng đỉnh\n",
    "hidden_dim = 64\n",
    "output_dim = 2\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "\n",
    "model = GraphNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Huấn luyện và kiểm thử mô hình\n",
    "for epoch in range(epochs):\n",
    "    train(model, train_data, criterion, optimizer)\n",
    "    train_accuracy, train_loss = evaluate(model, train_data, criterion)\n",
    "    test_accuracy, test_loss = evaluate(model, test_data, criterion)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# # Huấn luyện mô hình\n",
    "# def train(model, train_loader, criterion, optimizer, device):\n",
    "#     model.train()\n",
    "#     for data in train_loader:\n",
    "#         data = data.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = criterion(output, data.y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Đánh giá mô hình\n",
    "# def evaluate(model, loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data in loader:\n",
    "#             data = data.to(device)\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, data.y)\n",
    "#             total_loss += loss.item()\n",
    "#             _, predicted = torch.max(output, 1)\n",
    "#             correct += (predicted == data.y).sum().item()\n",
    "#             total_samples += data.y.size(0)\n",
    "\n",
    "#     accuracy = correct / total_samples\n",
    "#     average_loss = total_loss / len(loader)\n",
    "\n",
    "#     return accuracy, average_loss\n",
    "\n",
    "# # Thiết lập các tham số\n",
    "# input_dim = 75 # Thay thế bằng kích thước của vectơ đặc trưng đỉnh\n",
    "# hidden_dim = 64\n",
    "# output_dim = 2\n",
    "# lr = 0.001\n",
    "# epochs = 100\n",
    "\n",
    "# # Tạo mô hình và các thành phần khác\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GraphNeuralNetwork(input_dim, hidden_dim, output_dim).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# # Huấn luyện và kiểm thử mô hình\n",
    "# for epoch in range(epochs):\n",
    "#     train(model, train_loader, criterion, optimizer, device)\n",
    "#     train_accuracy, train_loss = evaluate(model, train_loader, criterion, device)\n",
    "#     test_accuracy, test_loss = evaluate(model, test_loader, criterion, device)\n",
    "#     print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Test Acc: {test_accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
