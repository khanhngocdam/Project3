{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84768 entries, 0 to 84767\n",
      "Data columns (total 85 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   FlowID                   84767 non-null  object \n",
      " 1   SourceIP                 84768 non-null  object \n",
      " 2   SourcePort               84768 non-null  int64  \n",
      " 3   DestinationIP            84768 non-null  object \n",
      " 4   DestinationPort          84768 non-null  int64  \n",
      " 5   Protocol                 84768 non-null  int64  \n",
      " 6   Timestamp                84768 non-null  object \n",
      " 7   FlowDuration             84768 non-null  int64  \n",
      " 8   TotalFwdPackets          84768 non-null  int64  \n",
      " 9   TotalBackwardPackets     84768 non-null  int64  \n",
      " 10  TotalLengthofFwdPackets  84768 non-null  float64\n",
      " 11  TotalLengthofBwdPackets  84768 non-null  float64\n",
      " 12  FwdPacketLengthMax       84768 non-null  float64\n",
      " 13  FwdPacketLengthMin       84768 non-null  float64\n",
      " 14  FwdPacketLengthMean      84768 non-null  float64\n",
      " 15  FwdPacketLengthStd       84768 non-null  float64\n",
      " 16  BwdPacketLengthMax       84768 non-null  float64\n",
      " 17  BwdPacketLengthMin       84768 non-null  float64\n",
      " 18  BwdPacketLengthMean      84768 non-null  float64\n",
      " 19  BwdPacketLengthStd       84768 non-null  float64\n",
      " 20  FlowBytes/s              84768 non-null  float64\n",
      " 21  FlowPackets/s            84768 non-null  float64\n",
      " 22  FlowIATMean              84768 non-null  float64\n",
      " 23  FlowIATStd               84768 non-null  float64\n",
      " 24  FlowIATMax               84768 non-null  float64\n",
      " 25  FlowIATMin               84768 non-null  float64\n",
      " 26  FwdIATTotal              84768 non-null  float64\n",
      " 27  FwdIATMean               84768 non-null  float64\n",
      " 28  FwdIATStd                84768 non-null  float64\n",
      " 29  FwdIATMax                84768 non-null  float64\n",
      " 30  FwdIATMin                84768 non-null  float64\n",
      " 31  BwdIATTotal              84768 non-null  float64\n",
      " 32  BwdIATMean               84768 non-null  float64\n",
      " 33  BwdIATStd                84768 non-null  float64\n",
      " 34  BwdIATMax                84768 non-null  float64\n",
      " 35  BwdIATMin                84768 non-null  float64\n",
      " 36  FwdPSHFlags              84768 non-null  int64  \n",
      " 37  BwdPSHFlags              84768 non-null  int64  \n",
      " 38  FwdURGFlags              84768 non-null  int64  \n",
      " 39  BwdURGFlags              84768 non-null  int64  \n",
      " 40  FwdHeaderLength          84768 non-null  int64  \n",
      " 41  BwdHeaderLength          84768 non-null  int64  \n",
      " 42  FwdPackets/s             84768 non-null  float64\n",
      " 43  BwdPackets/s             84768 non-null  float64\n",
      " 44  MinPacketLength          84768 non-null  float64\n",
      " 45  MaxPacketLength          84768 non-null  float64\n",
      " 46  PacketLengthMean         84768 non-null  float64\n",
      " 47  PacketLengthStd          84768 non-null  float64\n",
      " 48  PacketLengthVariance     84768 non-null  float64\n",
      " 49  FINFlagCount             84768 non-null  int64  \n",
      " 50  SYNFlagCount             84768 non-null  int64  \n",
      " 51  RSTFlagCount             84768 non-null  int64  \n",
      " 52  PSHFlagCount             84768 non-null  int64  \n",
      " 53  ACKFlagCount             84768 non-null  int64  \n",
      " 54  URGFlagCount             84768 non-null  int64  \n",
      " 55  CWEFlagCount             84768 non-null  int64  \n",
      " 56  ECEFlagCount             84768 non-null  int64  \n",
      " 57  Down/UpRatio             84768 non-null  object \n",
      " 58  AveragePacketSize        84767 non-null  float64\n",
      " 59  AvgFwdSegmentSize        84767 non-null  float64\n",
      " 60  AvgBwdSegmentSize        84767 non-null  float64\n",
      " 61  FwdHeaderLength.1        84767 non-null  float64\n",
      " 62  FwdAvgBytes/Bulk         84767 non-null  float64\n",
      " 63  FwdAvgPackets/Bulk       84767 non-null  float64\n",
      " 64  FwdAvgBulkRate           84767 non-null  float64\n",
      " 65  BwdAvgBytes/Bulk         84767 non-null  float64\n",
      " 66  BwdAvgPackets/Bulk       84767 non-null  float64\n",
      " 67  BwdAvgBulkRate           84767 non-null  float64\n",
      " 68  SubflowFwdPackets        84767 non-null  float64\n",
      " 69  SubflowFwdBytes          84767 non-null  float64\n",
      " 70  SubflowBwdPackets        84767 non-null  float64\n",
      " 71  SubflowBwdBytes          84767 non-null  float64\n",
      " 72  Init_Win_bytes_forward   84767 non-null  float64\n",
      " 73  Init_Win_bytes_backward  84767 non-null  float64\n",
      " 74  act_data_pkt_fwd         84767 non-null  float64\n",
      " 75  min_seg_size_forward     84767 non-null  float64\n",
      " 76  ActiveMean               84767 non-null  float64\n",
      " 77  ActiveStd                84767 non-null  float64\n",
      " 78  ActiveMax                84767 non-null  float64\n",
      " 79  ActiveMin                84767 non-null  float64\n",
      " 80  IdleMean                 84767 non-null  float64\n",
      " 81  IdleStd                  84767 non-null  float64\n",
      " 82  IdleMax                  84767 non-null  float64\n",
      " 83  IdleMin                  84767 non-null  float64\n",
      " 84  Label                    84768 non-null  int64  \n",
      "dtypes: float64(59), int64(21), object(5)\n",
      "memory usage: 55.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Đường dẫn đến folder chứa các file CSV\n",
    "folder_path_Benign2017 = './CSVS/Benign-CSVs/Benign/Benign2017'\n",
    "folder_path_Malware_Biige = './CSVS/SMSmalware-CSVs/SMSmalware/Biige'\n",
    "folder_path_Malware_Fakeinst = './CSVS/SMSmalware-CSVs/SMSmalware/Fakeinst'\n",
    "\n",
    "\n",
    "# Danh sách tất cả các file trong folder\n",
    "file_list = os.listdir(folder_path_Benign2017)\n",
    "file_list_malware_biige = os.listdir(folder_path_Malware_Biige)\n",
    "file_list_malware_fakeinst = os.listdir(folder_path_Malware_Fakeinst)\n",
    "# Lọc ra các file có đuôi là '.csv'\n",
    "csv_files = [file for file in file_list if file.endswith('.csv')]\n",
    "csv_files_malware_biige = [file for file in file_list_malware_biige if file.endswith('.csv')]\n",
    "csv_files_malware_fakeinst = [file for file in file_list_malware_fakeinst if file.endswith('.csv')]\n",
    "\n",
    "# Tạo một DataFrame để chứa dữ liệu từ tất cả các file CSV\n",
    "list_df = []\n",
    "all_data = pd.DataFrame()\n",
    "# Đọc từng file CSV và nối dữ liệu vào DataFrame chung\n",
    "i=0;\n",
    "for csv_file in csv_files:\n",
    "    if i < 60 :\n",
    "        file_path = os.path.join(folder_path_Benign2017, csv_file)\n",
    "        data = pd.read_csv(file_path, header=0)\n",
    "        data.columns = data.columns.str.replace(' ', '')\n",
    "        data = data.sort_values(by='Timestamp')\n",
    "        data['Label'] = 1\n",
    "        list_df.append(data)\n",
    "        all_data = pd.concat([all_data, data], ignore_index=True) \n",
    "        i = i + 1\n",
    "\n",
    "for csv_file in csv_files_malware_biige : \n",
    "    file_path = os.path.join(folder_path_Malware_Biige, csv_file)\n",
    "    data = pd.read_csv(file_path, header=0)\n",
    "    data.columns = data.columns.str.replace(' ', '')\n",
    "    data = data.sort_values(by='Timestamp')\n",
    "    data['Label'] = 0\n",
    "    list_df.append(data)\n",
    "    all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "for csv_file in csv_files_malware_fakeinst : \n",
    "    file_path = os.path.join(folder_path_Malware_Fakeinst, csv_file)\n",
    "    data = pd.read_csv(file_path, header=0)\n",
    "    data.columns = data.columns.str.replace(' ', '')\n",
    "    data = data.sort_values(by='Timestamp')\n",
    "    data['Label'] = 0\n",
    "    list_df.append(data)\n",
    "    all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Hiển thị DataFrame chứa dữ liệu từ tất cả các file CSV\n",
    "print(all_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    49124\n",
       "1    35644\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_valid_format(timestamp, format='%d/%m/%Y %H:%M:%S'):\n",
    "    try:\n",
    "        pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M:%S')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "list_df_5_minutes = []\n",
    "#Tạo list graph\n",
    "for df in list_df: \n",
    "    i = 0\n",
    "    timeStamp = df['Timestamp'].iloc[i] \n",
    "    # Chuyển đổi timestamp thành đối tượng datetime và cộng 5 phút\n",
    "    \n",
    "    while not is_valid_format(timeStamp, format='%d/%m/%Y %H:%M:%S') :\n",
    "        i = i + 1\n",
    "        timeStamp = df['Timestamp'].iloc[i]\n",
    "\n",
    "    datetime = pd.to_datetime(timeStamp, format='%d/%m/%Y %H:%M:%S') + pd.Timedelta(minutes=5)\n",
    "    new_df = pd.DataFrame(columns=all_data.columns)\n",
    "    for row in df.itertuples(index=False):\n",
    "        try:\n",
    "            date_time_of_row = pd.to_datetime(row.Timestamp, format='%d/%m/%Y %H:%M:%S')\n",
    "        except ValueError:\n",
    "            # Bỏ qua row nếu không thể chuyển đổi thành đối tượng datetime\n",
    "            continue\n",
    "        if(date_time_of_row <= datetime) : \n",
    "            new_df.loc[len(new_df)] = list(row)\n",
    "        elif not new_df.empty: \n",
    "            list_df_5_minutes.append(new_df)\n",
    "            new_df = pd.DataFrame(columns=all_data.columns)\n",
    "            datetime = datetime + pd.Timedelta(minutes=5)\n",
    "\n",
    "len(list_df_5_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ip_label(df):\n",
    "    le_columns = ['SourceIP', 'DestinationIP']\n",
    "    ip_list = list(df['SourceIP'].unique()) + list(df['DestinationIP'].unique())\n",
    "    ip_set = list(set(ip_list))\n",
    "\n",
    "    for column in le_columns:\n",
    "        list_unique = list(df[column].unique())\n",
    "        for val in list_unique:\n",
    "            df.loc[df[column] == val, column] = ip_set.index(val)\n",
    "\n",
    "    df['DestinationIP'] = df['DestinationIP'].astype(int)\n",
    "    df['SourceIP'] = df['SourceIP'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "list_graph = []\n",
    "list_label = []\n",
    "for df in list_df_5_minutes:\n",
    "    df = convert_ip_label(df)\n",
    "    graph = nx.from_pandas_edgelist(df, 'SourceIP', 'DestinationIP',\n",
    "                                     create_using=nx.MultiGraph(), edge_attr=df.drop(columns=['FlowID', 'SourceIP', 'DestinationIP', 'SourcePort', 'DestinationPort', 'Label', \n",
    "                                                                                              'Timestamp', 'PacketLengthStd', 'CWEFlagCount', 'Down/UpRatio']).columns.values.tolist())\n",
    "    list_graph.append(graph)\n",
    "    count_value = len(df['Label'].unique())\n",
    "    if count_value == 1: \n",
    "        label = df['Label'].unique()[0]\n",
    "    else: \n",
    "        label = 0\n",
    "    list_label.append(label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.loader import  DataLoader\n",
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, global_add_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Định nghĩa mô hình GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "        self.fc = nn.Linear(output_dim, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi và chia tập dữ liệu \n",
    "def graph_to_pyg_data(graph, label):\n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()[:-1]\n",
    "    x = torch.tensor([list(graph.edges[edge].values()) for edge in graph.edges], dtype=torch.float)\n",
    "    y = torch.tensor([label], dtype=torch.long)\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "list_graph_labeled = list(zip(list_graph, list_label))\n",
    "list_data = []\n",
    "for graph, label in list_graph_labeled:\n",
    "    list_data.append(graph_to_pyg_data(graph, label))\n",
    "    \n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "train_data, test_data = train_test_split(list_data, test_size=0.2, random_state=42)\n",
    "# Tạo DataLoader cho tập huấn luyện và tập kiểm tra\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 131236051.5000, Test Acc: 52.73%\n",
      "Epoch 2/100, Train Loss: 93359564.3750, Test Acc: 54.55%\n",
      "Epoch 3/100, Train Loss: 43698913.7500, Test Acc: 61.82%\n",
      "Epoch 4/100, Train Loss: 42138244.5000, Test Acc: 54.55%\n",
      "Epoch 5/100, Train Loss: 26227143.7500, Test Acc: 67.27%\n",
      "Epoch 6/100, Train Loss: 44898263.2500, Test Acc: 65.45%\n",
      "Epoch 7/100, Train Loss: 30341275.0000, Test Acc: 58.18%\n",
      "Epoch 8/100, Train Loss: 19422307.5000, Test Acc: 60.00%\n",
      "Epoch 9/100, Train Loss: 19529330.2500, Test Acc: 61.82%\n",
      "Epoch 10/100, Train Loss: 23706362.2500, Test Acc: 58.18%\n",
      "Epoch 11/100, Train Loss: 17668154.0000, Test Acc: 61.82%\n",
      "Epoch 12/100, Train Loss: 15745110.5000, Test Acc: 63.64%\n",
      "Epoch 13/100, Train Loss: 14909510.4375, Test Acc: 61.82%\n",
      "Epoch 14/100, Train Loss: 18660435.0000, Test Acc: 61.82%\n",
      "Epoch 15/100, Train Loss: 12060941.5000, Test Acc: 60.00%\n",
      "Epoch 16/100, Train Loss: 9985016.7812, Test Acc: 60.00%\n",
      "Epoch 17/100, Train Loss: 12438553.8750, Test Acc: 54.55%\n",
      "Epoch 18/100, Train Loss: 10051221.9375, Test Acc: 52.73%\n",
      "Epoch 19/100, Train Loss: 21223941.5000, Test Acc: 54.55%\n",
      "Epoch 20/100, Train Loss: 26255698.5000, Test Acc: 54.55%\n",
      "Epoch 21/100, Train Loss: 10888855.0000, Test Acc: 56.36%\n",
      "Epoch 22/100, Train Loss: 8434551.2500, Test Acc: 54.55%\n",
      "Epoch 23/100, Train Loss: 8602984.7500, Test Acc: 61.82%\n",
      "Epoch 24/100, Train Loss: 7951122.2500, Test Acc: 65.45%\n",
      "Epoch 25/100, Train Loss: 9537710.6875, Test Acc: 63.64%\n",
      "Epoch 26/100, Train Loss: 6997235.0000, Test Acc: 63.64%\n",
      "Epoch 27/100, Train Loss: 6693036.3750, Test Acc: 58.18%\n",
      "Epoch 28/100, Train Loss: 23853123.0000, Test Acc: 58.18%\n",
      "Epoch 29/100, Train Loss: 11749009.3750, Test Acc: 60.00%\n",
      "Epoch 30/100, Train Loss: 15552405.0000, Test Acc: 61.82%\n",
      "Epoch 31/100, Train Loss: 11267720.0000, Test Acc: 56.36%\n",
      "Epoch 32/100, Train Loss: 7797507.1250, Test Acc: 63.64%\n",
      "Epoch 33/100, Train Loss: 6148916.1875, Test Acc: 63.64%\n",
      "Epoch 34/100, Train Loss: 6619699.1250, Test Acc: 72.73%\n",
      "Epoch 35/100, Train Loss: 7557547.3750, Test Acc: 65.45%\n",
      "Epoch 36/100, Train Loss: 5621296.3125, Test Acc: 65.45%\n",
      "Epoch 37/100, Train Loss: 7256119.3750, Test Acc: 58.18%\n",
      "Epoch 38/100, Train Loss: 13336364.8750, Test Acc: 56.36%\n",
      "Epoch 39/100, Train Loss: 7157386.3125, Test Acc: 56.36%\n",
      "Epoch 40/100, Train Loss: 6512460.0625, Test Acc: 63.64%\n",
      "Epoch 41/100, Train Loss: 7399222.3750, Test Acc: 63.64%\n",
      "Epoch 42/100, Train Loss: 5914353.0625, Test Acc: 65.45%\n",
      "Epoch 43/100, Train Loss: 7002737.8750, Test Acc: 60.00%\n",
      "Epoch 44/100, Train Loss: 7687592.2500, Test Acc: 61.82%\n",
      "Epoch 45/100, Train Loss: 9547176.6250, Test Acc: 61.82%\n",
      "Epoch 46/100, Train Loss: 5723608.3750, Test Acc: 67.27%\n",
      "Epoch 47/100, Train Loss: 5665704.3125, Test Acc: 69.09%\n",
      "Epoch 48/100, Train Loss: 8479280.0625, Test Acc: 69.09%\n",
      "Epoch 49/100, Train Loss: 6569042.1250, Test Acc: 72.73%\n",
      "Epoch 50/100, Train Loss: 5920425.1250, Test Acc: 72.73%\n",
      "Epoch 51/100, Train Loss: 7166509.0625, Test Acc: 74.55%\n",
      "Epoch 52/100, Train Loss: 5924391.4375, Test Acc: 70.91%\n",
      "Epoch 53/100, Train Loss: 8239490.5000, Test Acc: 67.27%\n",
      "Epoch 54/100, Train Loss: 6899442.9375, Test Acc: 65.45%\n",
      "Epoch 55/100, Train Loss: 8986300.0000, Test Acc: 61.82%\n",
      "Epoch 56/100, Train Loss: 9826505.7500, Test Acc: 61.82%\n",
      "Epoch 57/100, Train Loss: 5617376.8438, Test Acc: 67.27%\n",
      "Epoch 58/100, Train Loss: 7157604.8750, Test Acc: 61.82%\n",
      "Epoch 59/100, Train Loss: 11556361.1250, Test Acc: 61.82%\n",
      "Epoch 60/100, Train Loss: 6313606.3125, Test Acc: 70.91%\n",
      "Epoch 61/100, Train Loss: 4931144.2656, Test Acc: 72.73%\n",
      "Epoch 62/100, Train Loss: 10397935.8750, Test Acc: 70.91%\n",
      "Epoch 63/100, Train Loss: 5022332.5938, Test Acc: 67.27%\n",
      "Epoch 64/100, Train Loss: 7280256.0000, Test Acc: 69.09%\n",
      "Epoch 65/100, Train Loss: 9590849.0000, Test Acc: 63.64%\n",
      "Epoch 66/100, Train Loss: 6686781.7500, Test Acc: 63.64%\n",
      "Epoch 67/100, Train Loss: 14307313.8750, Test Acc: 61.82%\n",
      "Epoch 68/100, Train Loss: 10213443.8750, Test Acc: 65.45%\n",
      "Epoch 69/100, Train Loss: 6749808.5000, Test Acc: 65.45%\n",
      "Epoch 70/100, Train Loss: 6527953.7500, Test Acc: 72.73%\n",
      "Epoch 71/100, Train Loss: 5658278.6250, Test Acc: 72.73%\n",
      "Epoch 72/100, Train Loss: 7288364.0000, Test Acc: 67.27%\n",
      "Epoch 73/100, Train Loss: 11846493.5000, Test Acc: 63.64%\n",
      "Epoch 74/100, Train Loss: 4462207.5625, Test Acc: 67.27%\n",
      "Epoch 75/100, Train Loss: 5002921.0625, Test Acc: 67.27%\n",
      "Epoch 76/100, Train Loss: 7767672.0000, Test Acc: 69.09%\n",
      "Epoch 77/100, Train Loss: 5750739.5625, Test Acc: 69.09%\n",
      "Epoch 78/100, Train Loss: 4194503.0938, Test Acc: 65.45%\n",
      "Epoch 79/100, Train Loss: 7964607.7500, Test Acc: 60.00%\n",
      "Epoch 80/100, Train Loss: 5123349.6250, Test Acc: 67.27%\n",
      "Epoch 81/100, Train Loss: 6987510.3750, Test Acc: 65.45%\n",
      "Epoch 82/100, Train Loss: 8705033.5625, Test Acc: 61.82%\n",
      "Epoch 83/100, Train Loss: 5419550.0000, Test Acc: 74.55%\n",
      "Epoch 84/100, Train Loss: 4840791.3750, Test Acc: 74.55%\n",
      "Epoch 85/100, Train Loss: 8082951.7500, Test Acc: 72.73%\n",
      "Epoch 86/100, Train Loss: 4697698.2500, Test Acc: 74.55%\n",
      "Epoch 87/100, Train Loss: 4204108.0000, Test Acc: 74.55%\n",
      "Epoch 88/100, Train Loss: 5525781.7500, Test Acc: 74.55%\n",
      "Epoch 89/100, Train Loss: 4172332.9375, Test Acc: 70.91%\n",
      "Epoch 90/100, Train Loss: 4389020.7188, Test Acc: 70.91%\n",
      "Epoch 91/100, Train Loss: 10861459.3750, Test Acc: 58.18%\n",
      "Epoch 92/100, Train Loss: 4416386.5000, Test Acc: 70.91%\n",
      "Epoch 93/100, Train Loss: 4337208.1875, Test Acc: 76.36%\n",
      "Epoch 94/100, Train Loss: 11014990.7500, Test Acc: 61.82%\n",
      "Epoch 95/100, Train Loss: 3848359.1250, Test Acc: 74.55%\n",
      "Epoch 96/100, Train Loss: 3596549.0938, Test Acc: 76.36%\n",
      "Epoch 97/100, Train Loss: 10301696.8750, Test Acc: 60.00%\n",
      "Epoch 98/100, Train Loss: 4729472.5625, Test Acc: 69.09%\n",
      "Epoch 99/100, Train Loss: 4063182.3906, Test Acc: 76.36%\n",
      "Epoch 100/100, Train Loss: 3199832.1875, Test Acc: 76.36%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Huấn luyện mô hình\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Đánh giá mô hình\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            total_samples += data.y.size(0)\n",
    "\n",
    "    accuracy = correct / total_samples\n",
    "    average_loss = total_loss / len(loader)\n",
    "\n",
    "    return accuracy, average_loss\n",
    "\n",
    "# Thiết lập các tham số\n",
    "input_dim = 75 # Thay thế bằng kích thước của vectơ đặc trưng đỉnh\n",
    "hidden_dim = 64\n",
    "output_dim = 2\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Tạo mô hình và các thành phần khác\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(input_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Huấn luyện và kiểm thử mô hình\n",
    "for epoch in range(epochs):\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    train_accuracy, train_loss = evaluate(model, train_loader, criterion, device)\n",
    "    test_accuracy, test_loss = evaluate(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Test Acc: {test_accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
